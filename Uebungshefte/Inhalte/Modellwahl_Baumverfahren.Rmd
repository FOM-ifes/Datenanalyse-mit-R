# Kapitel `r kap <- kap+1; kap`: Baumverfahren und Modellwahl in R

## Grundlagen Baum-basierter Verfahren

Laut Breimann (2001)^[vgl. Breiman, L. (2001). Statistical modeling: The two cultures. Statistical Science, 16(3), 199-231.] gibt es zwei verschiedene *Kulturen* um einen Zusammenhang zwischen einer abhängigen Variable $y$ und einer oder mehrerer unabhängiger $x_i$ zu schätzen:

$$y=f(x)+\epsilon$$

- *Data Modeling*: Annahme über die Funktion $f$, Schätzung von unbekannten Modellparametern mit Hilfe einer Stichprobe: z. B. lineare oder logistische Regression. 
- *Algorithmic Modeling*: $f$ unbekannt, algorithmische Bestimmung des Zusammenhangs von $x$ und $y$ anhand der vorliegenden Daten: z. B. Baum-basierte Verfahren.

Dabei sollten wir laut Hand (2014)^[Hand, D. J. (2014). Wonderful Examples, but Let’s not Close Our Eyes. Statistical Science 29(1)(2014), 98-100] folgendes nicht vergessen:

> [...] In general, when building statistical models, we must not forget that the aim is to understand something about the real world. Or predict, choose an action, make a decision, summarize evidence, and so on, but always about the real world, not an abstract mathematical world: our models are not the reality -- a point well made by George Box in his oft-cited remark that 'all models are wrong, but some are useful' [...]

Dabei stellen sich natürlich Fragen:

- Wie kann das "richtige Modell" $f$ gefunden werden, d. h., wie kann $f$ beurteilt werden?
- Es existieren viele Gütemaße für ein (statistisches) Modell: z. B. Bestimmtheitsmaß $R^2$, AIC, BIC, AUC. **Keines** garantiert, dass das "richtige" Modell gefunden wird
- Vorhersagen sind (auch) wichtig. Wie schon Søren Kierkegaard sagte: "Es ist ganz wahr, was die Philosophie sagt, daß das Leben rückwärts verstanden werden muß. Aber darüber vergißt man den andern Satz, daß vorwärts gelebt werden muß."^[Søren Kierkegaard. Die Tagebücher. Deutsch von Theodor Haecker. Brenner-Verlag 1923, S. 203 [https://books.google.de/books?id=8D-ehrAsTPAC&q=r%C3%BCckw%C3%A4rts&hl=de](https://books.google.de/books?id=8D-ehrAsTPAC&q=r%C3%BCckw%C3%A4rts&hl=de)] 

(Ein) Kriterium zur Beurteilung eines Modells ist der (Test) Mean Squared Error (MSE):
$$MSE=E(y_0-\hat{y}_0)^2=Var(\hat{y}_0)+Bias^2(\hat{y}_0)+Var(\epsilon)$$

- Varianz: Streuung in der Prognose bedingt durch Varianz der Trainingsdaten – je flexibler/ komplexer ein Modell desto größer
- Bias$^2$: Abweichung$^2$ zwischen wahren Wert und Schätzwert, z. B.  bedingt durch Modellfehler – je flexibler/ komplexer ein Modell desto kleiner

Dabei weiß man nicht, bei welcher Komplexität der Mean Squared Error minimal ist!

*Regressions- und Klassifikationsbäume* (CART, Classification and Regression Trees) funktionieren ähnlich wie Entscheidungsbäume ("recursive binary splitting"): Ausgehend von einer Wurzel (root) werden Äste (branches) anhand eines Aufteilungskriteriums (splitting criterion) gebildet (Basis $x$), bis schließlich das Ergebnis (Prognose für $y$) in den Blättern (terminal node; leaf) steht. Die Verzweigungen innerhalb des Baumes werden Knoten (internal nodes) genannt.
Dabei werden die Werte der abhängigen Variable in den Ästen bzw. Blättern immer einheitlicher (impurity $\rightarrow$ purity)

Prinzipiell könnte der Baum jede einzelne Beobachtung als Blatt enthalten: Überanpassung (overfitting). Daher muss der Baum beschnitten werden (pruning). Dies erfolgt mit Hilfe eines Komplexitätsparameters der über Kreuzvalidierung bestimmt wird (siehe z. B. Therneau & Atkinson, 2017)^[[https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)] 

Eine solche Aufteilung kann z. B. dann wie folgt aussehen:

```{r, echo=FALSE, message=FALSE}
library(klaR)
data(B3)
partimat(PHASEN~LSTKJW+EWAJW, data=B3, method="rpart") 
```

## Konjunkturanalyse
Der B3 Datensatz *Heilemann, U. and Münch, H.J. (1996): West German Business Cycles 1963-1994: A Multivariate Discriminant Analysis. CIRET–Conference in Singapore, CIRET–Studien 50.* enthält Quartalsweise Konjunkturdaten aus (West-)Deutschland.

Er kann von [https://goo.gl/0YCEHf](https://goo.gl/0YCEHf) heruntergeladen werden:
```{r}
download.file("https://goo.gl/0YCEHf", destfile = "B3.csv")
```
Anschließend können die Daten in R eingelesen werden:
```{r}
B3 <- read.csv2("B3.csv")
str(B3) # Datenstruktur
head(B3); tail(B3)
```

Dabei sind folgende Variablen enthalten:

- Bruttosozialprodukt (real): `BSP91JW`
- Privater Verbrauch (real): `CP91JW`
- Anteil Staatsdefizit am Bruttosozialprodukt (%): `DEFRATE`
- Abhängig Erwerbstätige: `EWAJW`
- Anteil Außenbeitrag am Bruttosozialprodukt (%): `EXIMRATE`
- Geldmenge M1: `GM1JW`
- Investitionen in Ausrüstungsgüter (real): `IAU91JW`
- Investitionen in Bauten (real): `IB91JW`
- Lohnstückkosten: `LSTKJW`
- Preisindex des Bruttosozialprodukts: `PBSPJW`
- Preisindex des privaten Verbrauchs: `PCPJW`
- Kurzfristiger Zinssatz (nominal): `ZINSK`
- Langfristiger Zinssatz (real): `ZINSLR`
- Konjunkturphase: 1. Aufschwung, 2. Oberer Wendepunkt, 3. Abschwung, 4. Unterer Wendepunkt: `PHASEN`

Variablen mit der Endung *JW* beziehen sich auf die jährliche Veränderung.

## Regressionsbäume

Regressionsbäume werden bei einer metrischen Zielvariable $y$ verwendet. Dabei erfolgt das Splitting anhand von ANOVA Überlegungen: Teile die Wurzel, den Ast (internal node) anhand der Variable $x_j$ an der Stelle $s$ so auf, dass die Fehlerquadratsumme (RSS) in den resultierenden Halbräumen minimiert wird. Dabei wird zur Prognose von $y$ der arithmetische Mittelwert des jeweiligen Halbraums verwendet.

Um einen Regressionsbaum zu erzeugen, muss zunächst das Zusatzpaket `rpart` geladen werden:
```{r, message=FALSE}
library(rpart)
```

Um z. B. die Veränderung des Bruttosozialprodukt als Funktion von Privater Verbrauch, Investitionen in Ausrüstungsgüter, Investitionen in Bauten und Geldmenge M1 als Regressionsbaum zu modellieren, reicht der Befehl
```{r}
regbaum <- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3)
```


Um das Ergebnis auszugeben, genügt:
```{r}
regbaum
```
*Lesebeispiel:* Wenn CP91JW>=3.705 und IAU91JW>=11.335 und IB91JW>=7.55 liegt, dann liegt die durchschnittliche Veränderung des BSP91JW bei 8.639. 11 Beobachtungen erfüllen diese Kriterien in den unabhängigen Variablen

Bzw. um den Baum zu zeichnen

```{r}
par(xpd = TRUE) # Grafikparameter der sicherstellt, dass alles ins Bild passt
plot(regbaum, compress = TRUE) # Baum zeichnen
text(regbaum) # Baum beschriften
```


Eine deutlich schönere Ausgabe erhält man z. B. mit dem Zusatzpaket `rpart.plot`, welches *einmalig* über
```{r, eval=FALSE}
install.packages("rpart.plot")
```
installiert werden muss und dann benutzt werden kann.

Zunächst laden
```{r, message=FALSE}
library(rpart.plot)

```
und dann zeichnen:
```{r}
rpart.plot(regbaum)
```

### Prognose
Erzeugen Sie einen `data.frame` mit den Werten für die Prognose und verwenden die Funktion `predict`.
```{r}
newdat <- data.frame(CP91JW = 5, IAU91JW = 2, IB91JW = 10, GM1JW = 5)
predict(regbaum, newdata = newdat)

```




### Kreuzvalidierung

#### Anpassungsgüte
Wie gut ist das Modell? Über `predict()` können die Punktprognosen berechnet werden:
```{r}
head(predict(regbaum))
```
Diese werden mit den beobachteten Werten verglichen:
```{r}
head(B3$BSP91JW)
```

Der **Mean Squared Error** ist dann
```{r}
baummse <- mean( (predict(regbaum) - B3$BSP91JW)^2 )
baummse
```

Vergleichen wir das Ergebnis mit dem einer linearen Regression
```{r}
reglm <- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3)
summary(reglm)
```

Der MSE der Linearen Regression liegt bei
```{r}
lmmse <- mean( (predict(reglm) - B3$BSP91JW)^2 )
lmmse
```

Der Baum ist einfacher, aber auch schlechter im Bezug auf die Anpassungsgüte.

#### Prognosegüte
Für eine k=3 fache *Kreuzvalidierung* müssen 3 Testdatensätze erzeugt werden.

Zunächst wird dafür ein Aufteilungsvektor gebildet:
```{r}
aufteilung <- rep(1:3, length.out=nrow(B3))
```
und dann wird aufgeteilt:
```{r}
test1 <- B3[aufteilung==1,] # Nur die 1, 4, 7, ... Beobachtung
train1 <- B3[aufteilung!=1,] # Ohne die 1, 4, 7, ... Beobachtung

test2 <- B3[aufteilung==2,] # Nur die 2, 5, 8, ... Beobachtung
train2 <- B3[aufteilung!=2,] # Ohne die 2, 5, 8, ... Beobachtung

test3 <- B3[aufteilung==3,] # Nur die 3, 6, 9, ... Beobachtung
train3 <- B3[aufteilung!=3,] # Ohne die 3, 6, 9, ... Beobachtung
```

Anschließend werden die Modelle auf den Trainingsdaten geschätzt, und auf den Testdaten überprüft:
```{r}
# Runde 1
b1 <- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1)
l1 <- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1)
mseb1 <- mean( (predict(b1, newdata = test1) - test1$BSP91JW)^2 )
msel1 <- mean( (predict(l1, newdata = test1) - test1$BSP91JW)^2 )

# Runde 2
b2 <- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2)
l2 <- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2)
mseb2 <- mean( (predict(b2, newdata = test2) - test2$BSP91JW)^2 )
msel2 <- mean( (predict(l2, newdata = test2) - test2$BSP91JW)^2 )

# Runde 3
b3 <- rpart(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3)
l3 <- lm(BSP91JW ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3)
mseb3 <- mean( (predict(b3, newdata = test3) - test3$BSP91JW)^2 )
msel3 <- mean( (predict(l3, newdata = test3) - test3$BSP91JW)^2 )

# Ergebnisse zusammenfassen
msecvb <- c(mseb1, mseb2, mseb3)
msecvl <- c(msel1, msel2, msel3)

# Mittelwert des Prognose MSE
mean(msecvb)
mean(msecvl)
```

Bei den vorliegenden Daten ist also ein *lineares* Modell dem Baummodell im Bezug auf den *MSE* überlegen.

**Hinweis:** In der Praxis führt man die Aufteilung nicht manuell sondern innerhalb von Schleifen durch.


## Klassifikationbäume

Klassifikationsbäume werden für eine nominale abhängige Variable $y$ mit $K$ Ausprägungen verwendet. Das Splitting erfolgt z. B. anhand von Einheitlichkeit (purity) Überlegungen: Teile die Wurzel, den Ast (internal node) anhand der Variable $x_j$ an der Stelle $s$ so auf, dass der Gini Index in den resultierenden Halbräumen minimiert wird. Dabei wird zur Prognose von $y$ der Modalwert des jeweiligen Halbraums verwendet

Untersuchen wir, ob sich makroökonomische Kennzahlen geeignet sind, die Konjunkturphasen zu unterscheiden. 
Zunächst stellen wir fest, dass die eigentlich kategorielle Variable `PHASEN` hier numerisch kodiert wurde, was aber schnell verwirren würde.
```{r}
typeof(B3$PHASEN)
```
Typänderung zu `factor` geht einfach:
```{r}
B3$PHASEN <- as.factor(B3$PHASEN)
```
Wenn wir die einzelnen `levels` des Faktors als numerische Werte verwenden wollen würde man den Befehl `as.numeric()` verwenden. Aber sicherheitshalber vorher über `levels()` gucken, ob die Reihenfolge auch stimmt.

Um die Interpretation zu erleichtern können wir hier einfach die Faktorstufe umbenennen.
```{r}
levels(B3$PHASEN) <- c("Up", "utp", "Down", "ltp")
```

Um z. B. die Konjunkturphase als Funktion von Privater Verbrauch, Investitionen in Ausrüstungsgüter, Investitionen in Bauten und Geldmenge M1 als Regressionsbaum zu modellieren reicht jetzt der Befehl
```{r}
klassbaum <- rpart(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3)
```

Um das Ergebnis auszugeben genügt:
```{r}
klassbaum
```

*Lesebeispiel:* Wenn `IAU91JW`$< -0.09$ und `GM1JW`$\ge11.355$ liegt, dann ist der Untere Wendepunkt die häufigste Merkmalsausprägung von PHASEN (relative Häufigkeit von PHASEN=4 hier: 0.7) 10 Beobachtungen erfüllen die Kriterien der unabhängigen Variablen.


```{r}
par(xpd = TRUE) # Grafikparameter der sicherstellt, dass alles ins Bild passt
plot(klassbaum, compress = TRUE) # Baum zeichnen
text(klassbaum) # Baum beschriften
```

Bzw. "schöner":
```{r}
rpart.plot(klassbaum)
```


### Kreuzvalidierung

Wie gut ist das Modell? Auch hier können über `predict()` die Punktprognosen bestimmt werden:
```{r}
head(predict(klassbaum, type="class"))
```
Diese werden mit den beobachteten Werten verglichen:
```{r}
head(B3$PHASEN)
```

Die **Fehlklassifikationsrate** ist dann
```{r}
baumer <- mean( (predict(klassbaum, type="class") != B3$PHASEN) )
baumer
```
also knapp 30\%.

Vergleichen kann ein Klassifikationsbaum z. B. mit der *Linearen Diskriminanzanalyse, LDA*. Diese ist als Funktion `lda()` im Paket `MASS` implementiert. Bei einer LDA wird von einer klassenweisen (d. h. hier je Konjunkturphase) multivariaten Normalverteilung ausgegangen, wobei sich je Klasse der Mittelwertsvektor $\mu$ unterscheidet, die Kovarianzmatrix $\Sigma$ aber für alle Klassen gleich ist. Unterschiedliche a-priori Wahrscheinlichkeiten werden berücksichtigt.

```{r, message=FALSE}
library(MASS)
```

```{r}
klasslda <- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=B3)
klasslda

ldaer <- mean( (predict(klasslda)$class != B3$PHASEN) )
ldaer
```
Im Bezug auf die *Klassifikation* scheint der Baum in der Anpassungsgüte besser als die Lineare Diskriminanzanalyse zu sein. Aber wie sieht es kreuzvalidiert, d. h. in der Prognose aus?


Zunächst wird wieder dafür ein Aufteilungsvektor gebildet:
```{r}
aufteilung <- rep(1:3, length.out=nrow(B3))
```
und dann wird aufgeteilt:
```{r}
test1 <- B3[aufteilung==1,]
train1 <- B3[aufteilung!=1,]

test2 <- B3[aufteilung==2,]
train2 <- B3[aufteilung!=2,]

test3 <- B3[aufteilung==3,]
train3 <- B3[aufteilung!=3,]
```

```{r}
# Runde 1
b1 <- rpart(PHASEN  ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1)
l1 <- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train1)
erb1 <- mean( (predict(b1, newdata = test1, type = "class") != test1$PHASEN) )
erl1 <- mean( (predict(l1, newdata = test1)$class  != test1$PHASEN) )

# Runde 2
b2 <- rpart(PHASEN  ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2)
l2 <- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train2)
erb2 <- mean( (predict(b2, newdata = test2, type = "class") != test2$PHASEN) )
erl2 <- mean( (predict(l2, newdata = test2)$class  != test2$PHASEN) )

# Runde 3
b3 <- rpart(PHASEN  ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3)
l3 <- lda(PHASEN ~ CP91JW + IAU91JW + IB91JW + GM1JW, data=train3)
erb3 <- mean( (predict(b3, newdata = test3, type = "class") != test3$PHASEN) )
erl3 <- mean( (predict(l3, newdata = test3)$class  != test3$PHASEN) )

# Ergebnisse zusammenfassen
ercvb <- c(erb1, erb2, erb3)
ercvl <- c(erl1, erl2, erl3)

# Mittelwert des Prognose MSE
mean(ercvb)
mean(ercvl)
```
In der *Prognosegüte* ist hier -- anders als in der Anpassungsgüte -- die Lineare Diskriminanzanalyse besser.

Die Funktion `partimat()` aus dem Paket `klaR` ermöglicht einen visuellen Vergleich der Auteilungen von `lda()` und `rpart()`, hier nur für die Variablen `CP91JW + IAU91JW`. `app. error rate` ist die (Trainings-)Fehlklassifikationsrate mit den zwei unabhängigen Variablen.

```{r}
# Einmalig installieren
# install.packages("klaR") 

library(klaR)
partimat(PHASEN~CP91JW + IAU91JW, data=B3, method="rpart", main = "Partition Plot rpart")
partimat(PHASEN~CP91JW + IAU91JW, data=B3, method="lda", main = "Partition Plot lda")
```


Es ist deutlich zu erkennen, dass beim Klassifikationsbaum die Klassengrenzen hierarisch und parallel zu den Achsen verlaufen, während diese bei der Linearen Diskriminanzanalyse zwar linear, aber nicht parallel zu den Achsen sind. 

*Ergänzung:* Wird bei einer Linearen Diskriminanzanalyse die Annahme der gleichen Kovarianzen innerhalb der Klassen fallengelassen, kann eine *Quadratische Diskriminanzanalyse, QDA* verwendet werden. Diese ist dann in den Klassengrenzen wieder flexibler:
```{r}
partimat(PHASEN~CP91JW + IAU91JW, data=B3, method="qda", main = "Partition Plot qda")
```


## Parameter `rpart()` 
Neben dem Splitkrierium können verschiedene Parameter des Algorithmus eingestellt werden (siehe `?rpart.control`), u. a.:

- `minsplit`: Minimale Anzahl Beobachtungen im Knoten damit Aufteilung versucht wird
- `minbucket`: Minimale Anzahl Beobachtungen im Blatt
- `cp`: Komplexitätsparameter (pruning)
- `xval`: Anzahl Kreuzvaliderungen (pruning)
- `maxdepth`: Maximale Tiefe eines Blattes

Diese können mit der Funktion `train()` aus dem Paket [`caret`](https://topepo.github.io/caret/index.html) automatisch optimiert werden.

Alternativen/ Ergänzungen zu `rpart`: 

- [`tree`](https://cran.r-project.org/web/packages/tree/)
- [`partykit`](http://partykit.r-forge.r-project.org/partykit/)
- Erweiterung: Viele Bäume: [`randomForest`](https://cran.r-project.org/web/packages/randomForest/) 

## Vor- und Nachteile baumbasierter Verfahren

Vorteile:

- leicht zu erklären und zu interpretieren
- Evt. spiegeln sie menschliches Verhalten wieder
- Robust gegen Ausreißer
- Bimodale/ Nicht-monotone Zusammenhänge können dargestellt werden
- Qualitative und quantitative Variablen können direkt verwendet werden, fehlende Werte ggfs. als eigene Merkmalsausprägung
- Integrierte Variablenselektion

Nachteile: 

- Nicht optimal im Sinne eines Zielkriteriums (i. d. R. existieren "bessere" Verfahren)
- Hängen von "Parametern" ab (siehe Abschnitt rpart Parameter)
- Evt. instabil (kleine Änderungen der Daten können zu völlig anderen Bäumen führen)
- Nur eindimensionale hierarchische Partitionen

## Literatur

- Leo Breiman (2001): *Statistical modeling: The two cultures.* Statistical Science, 16(3), 199--231. [https://projecteuclid.org/euclid.ss/1009213726](https://projecteuclid.org/euclid.ss/1009213726)
- David J. Hand (2014): *Wonderful Examples, but Let’s not Close Our Eyes.* Statistical Science 29(1), 98--100. [http://projecteuclid.org/euclid.ss/1399645735](http://projecteuclid.org/euclid.ss/1399645735)
- Trevor Hastie, Robert Tibshirani, Jerome H. Friedman, (2009): *The Elements of Statistical Learning*. Springer. [http://statweb.stanford.edu/~tibs/ElemStatLearn/](http://statweb.stanford.edu/~tibs/ElemStatLearn/)
- Max Kuhn(2008): *Building predictive models in R using the caret package.* Journal of Statistical Software, 28(5), 1--26. [https://www.jstatsoft.org/article/view/v028i05](https://www.jstatsoft.org/article/view/v028i05)
- Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani (2013): *An Introduction to Statistical Learning -- with Applications in R*, [http://www-bcf.usc.edu/~gareth/ISL/](http://www-bcf.usc.edu/~gareth/ISL/)
- Terry M. Therneau, Elizabeth J. Atkinson (2017): *An introduction to recursive partitioning using the RPART routines*. [https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf](https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf)
- Hal R. Varian (2014): *Big data: New tricks for econometrics.* The Journal of Economic Perspectives, 3--27. [https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3](https://www.aeaweb.org/articles?id=10.1257/jep.28.2.3)

### Lizenz

Diese Übung wurde von Karsten Lübke entwickelt und steht unter der Lizenz [Creative Commons Attribution-ShareAlike 3.0 Unported](http://creativecommons.org/licenses/by-sa/3.0).


### Versionshinweise:
* Datum erstellt: `r Sys.Date()`
* R Version: `r getRversion()`
* `rpart` Version: `r packageVersion("rpart")`
* `MASS` Version: `r packageVersion("MASS")`